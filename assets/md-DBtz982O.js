import{_ as d}from"./slidev/VClick-BAELB_jA.js";import{b as u,o as m,w as a,g as e,e as t,ad as n,v as c,x as g,T as o}from"./modules/vue-Bicpx2LX.js";import{I as f}from"./slidev/two-cols-header-CTNwUue_.js";import{u as _,f as h}from"./slidev/context-LoJ4-s6V.js";import{_ as b}from"./index-DkBDYXOT.js";import"./modules/shiki-C2XUjZyd.js";const x={__name:"slides.md__slidev_4",setup(k){const{$clicksContext:i,$frontmatter:r}=_();return i.setup(),(v,l)=>{const s=d;return m(),u(f,c(g(o(h)(o(r),3))),{left:a(p=>[...l[0]||(l[0]=[e("p",null,[n("Tokenization follows a systematic algorithm based on "),e("strong",null,"Byte Pair Encoding (BPE)"),n(". There are multiple algorithms are available and different LLM model uses different BPE algorithm.")],-1),e("h3",null,"BPE Algorithm Overview:",-1),e("ul",null,[e("li",null,"Start with individual characters"),e("li",null,"Find most frequent adjacent pairs"),e("li",null,"Merge these pairs into single tokens"),e("li",null,"Repeat until vocabulary size reached"),e("li",null,"Apply learned merges to new text")],-1)])]),right:a(p=>[t(s,null,{default:a(()=>[...l[1]||(l[1]=[e("h3",null,"Example:",-1),e("p",null,[n("Using the "),e("code",null,"tiktoken"),n(" as per "),e("code",null,"o200k-base"),n(" encoding, "),e("span",{class:"font-600 font-mono text-primary"},"My Name is Debanjan."),n(" is split into 7 tokens as shown below.")],-1),e("div",{class:"text-sm leading-none font-mono mb-6px text-center"},[e("span",{class:"leading-none bg-[hsla(260,100%,75%,0.6)] pl-2px"},"My"),e("span",{class:"leading-none bg-[hsla(135,100%,75%,0.6)]"}," Name"),e("span",{class:"leading-none bg-[hsla(40,100%,75%,0.6)]"}," is"),e("span",{class:"leading-none bg-[hsla(358,100%,75%,0.6)]"}," Deb"),e("span",{class:"leading-none bg-[hsla(195,100%,75%,0.6)]"},"anj"),e("span",{class:"leading-none bg-[hsla(260,100%,75%,0.6)]"},"an"),e("span",{class:"leading-none bg-[hsla(135,100%,75%,0.6)] pr-2px"},".")],-1)])]),_:1}),t(s,null,{default:a(()=>[...l[2]||(l[2]=[e("h3",null,"Key Observations:",-1),e("ul",null,[e("li",null,"One word can split into multiple tokens."),e("li",null,"Spaces are often attached to following words."),e("li",null,"Common words typically are 1 token."),e("li",null,"Rare words typically are multiple tokens."),e("li",null,"Punctuation and special characters are separate tokens.")],-1)])]),_:1})]),default:a(()=>[l[3]||(l[3]=e("h1",null,"Understanding Tokenization",-1))]),_:1},16)}}},N=b(x,[["__scopeId","data-v-694056e3"]]);export{N as default};
