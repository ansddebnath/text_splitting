import{b as a,o as l,w as r,g as e,ad as i,v as u,x as c,T as o}from"./modules/vue-Bicpx2LX.js";import{I as d}from"./slidev/default-SveKq60L.js";import{u as m,f as p}from"./slidev/context-LoJ4-s6V.js";import"./index-DkBDYXOT.js";import"./modules/shiki-C2XUjZyd.js";const C={__name:"slides.md__slidev_3",setup(f){const{$clicksContext:s,$frontmatter:n}=m();return s.setup(),(h,t)=>(l(),a(d,u(c(o(p)(o(n),2))),{default:r(()=>[...t[0]||(t[0]=[e("h1",null,"What Are Tokens?",-1),e("h2",null,"Definition",-1),e("p",null,'Tokens are the fundamental units that language models process. They represent the "vocabulary" that AI models understand.',-1),e("h2",null,"Token Characteristics:",-1),e("ul",null,[e("li",null,"Not equivalent to words or characters"),e("li",null,"Vary in length and composition"),e("li",null,"Model-specific (different models use different tokenization schemes)"),e("li",null,"The basis for all input/output limitations")],-1),e("p",null,[e("strong",null,"Critical Concept:"),i(" Language models donâ€™t read text as humans do. They process numerical token sequences.")],-1)])]),_:1},16))}};export{C as default};
