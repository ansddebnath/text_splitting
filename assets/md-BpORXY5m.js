import{_ as d}from"./slidev/VClick-BAELB_jA.js";import{b as a,o as i,w as n,g as t,e as m,ad as e,v as p,x as c,T as o}from"./modules/vue-Bicpx2LX.js";import{I as k}from"./slidev/default-SveKq60L.js";import{u as _,f}from"./slidev/context-LoJ4-s6V.js";import"./index-DkBDYXOT.js";import"./modules/shiki-C2XUjZyd.js";const w={__name:"slides.md__slidev_5",setup(x){const{$clicksContext:s,$frontmatter:u}=_();return s.setup(),(T,l)=>{const r=d;return i(),a(k,p(c(o(f)(o(u),4))),{default:n(()=>[l[1]||(l[1]=t("h1",null,"Why Tokens Matters",-1)),l[2]||(l[2]=t("ul",null,[t("li",null,[e("Every LLM has a fixed "),t("strong",null,"context window"),e(" - the maximum number of tokens it can process in a single request.")]),t("li",null,"Understanding encoding helps predict token behavior and overcome token limit issue.")],-1)),m(r,null,{default:n(()=>[...l[0]||(l[0]=[t("h2",null,"Model token limits",-1),t("table",null,[t("thead",null,[t("tr",null,[t("th",null,"Model"),t("th",null,"Context Window"),t("th",null,"Use Case")])]),t("tbody",null,[t("tr",null,[t("td",null,"GPT-3.5-Turbo"),t("td",null,"4,096 tokens"),t("td",null,"Short conversations")]),t("tr",null,[t("td",null,"GPT-4"),t("td",null,"8,192 tokens"),t("td",null,"Standard applications")]),t("tr",null,[t("td",null,"GPT-4-32K"),t("td",null,"32,768 tokens"),t("td",null,"Long documents")]),t("tr",null,[t("td",null,"GPT-4-Turbo"),t("td",null,"128,000 tokens"),t("td",null,"Complex analysis")]),t("tr",null,[t("td",null,"Claude 3.5 Sonnet"),t("td",null,"200,000 tokens"),t("td",null,"Extended context")]),t("tr",null,[t("td",null,"Gemini 1.5 Pro"),t("td",null,"1,000,000 tokens"),t("td",null,"Massive documents")])])],-1)])]),_:1})]),_:1},16)}}};export{w as default};
