import{a as d,b as u,o as m,w as a,g as e,e as s,ad as l,v as c,x as g,S as o}from"./modules/vue-CJzrtd1M.js";import{I as f}from"./slidev/two-cols-header-C2i9toq0.js";import{u as _,f as b}from"./slidev/context-CYMzQU4q.js";import{_ as h}from"./index-B5JIHzwN.js";import"./modules/shiki-BB3fXZko.js";const k={__name:"long_content_optimization.md__slidev_4",setup(x){const{$clicksContext:i,$frontmatter:r}=_();return i.setup(),(v,n)=>{const t=d("v-click");return m(),u(f,c(g(o(b)(o(r),3))),{left:a(p=>[...n[0]||(n[0]=[e("p",null,[l("Tokenization follows a systematic algorithm based on "),e("strong",null,"Byte Pair Encoding (BPE)"),l(". There are multiple algorithms are available and different LLM model uses different BPE algorithm.")],-1),e("h3",null,"BPE Algorithm Overview:",-1),e("ul",null,[e("li",null,"Start with individual characters"),e("li",null,"Find most frequent adjacent pairs"),e("li",null,"Merge these pairs into single tokens"),e("li",null,"Repeat until vocabulary size reached"),e("li",null,"Apply learned merges to new text")],-1)])]),right:a(p=>[s(t,null,{default:a(()=>[...n[1]||(n[1]=[e("h3",null,"Example:",-1),e("p",null,[l("Using the "),e("code",null,"tiktoken"),l(" as per "),e("code",null,"o200k-base"),l(" encoding, "),e("span",{class:"font-600 font-mono text-primary"},"My Name is Debanjan."),l(" is split into 7 tokens as shown below.")],-1),e("div",{class:"text-sm leading-none font-mono mb-6px text-center"},[e("span",{class:"leading-none bg-[hsla(260,100%,75%,0.6)] pl-2px"},"My"),e("span",{class:"leading-none bg-[hsla(135,100%,75%,0.6)]"}," Name"),e("span",{class:"leading-none bg-[hsla(40,100%,75%,0.6)]"}," is"),e("span",{class:"leading-none bg-[hsla(358,100%,75%,0.6)]"}," Deb"),e("span",{class:"leading-none bg-[hsla(195,100%,75%,0.6)]"},"anj"),e("span",{class:"leading-none bg-[hsla(260,100%,75%,0.6)]"},"an"),e("span",{class:"leading-none bg-[hsla(135,100%,75%,0.6)] pr-2px"},".")],-1)])]),_:1}),s(t,null,{default:a(()=>[...n[2]||(n[2]=[e("h3",null,"Key Observations:",-1),e("ul",null,[e("li",null,"One word can split into multiple tokens."),e("li",null,"Spaces are often attached to following words."),e("li",null,"Common words typically are 1 token."),e("li",null,"Rare words typically are multiple tokens."),e("li",null,"Punctuation and special characters are separate tokens.")],-1)])]),_:1})]),default:a(()=>[n[3]||(n[3]=e("h1",null,"Understanding Tokenization",-1))]),_:1},16)}}},C=h(k,[["__scopeId","data-v-97b4b148"]]);export{C as default};
