import{a,b as d,o as i,w as n,g as t,e as m,ad as e,v as p,x as c,S as o}from"./modules/vue-CJzrtd1M.js";import{I as k}from"./slidev/default-BA8wdPvG.js";import{u as x,f}from"./slidev/context-CYMzQU4q.js";import"./index-B5JIHzwN.js";import"./modules/shiki-BB3fXZko.js";const b={__name:"long_content_optimization.md__slidev_5",setup(_){const{$clicksContext:s,$frontmatter:u}=x();return s.setup(),(v,l)=>{const r=a("v-click");return i(),d(k,p(c(o(f)(o(u),4))),{default:n(()=>[l[1]||(l[1]=t("h1",null,"Why Tokens Matters",-1)),l[2]||(l[2]=t("ul",null,[t("li",null,[e("Every LLM has a fixed "),t("strong",null,"context window"),e(" - the maximum number of tokens it can process in a single request.")]),t("li",null,"Understanding encoding helps predict token behavior and overcome token limit issue.")],-1)),m(r,null,{default:n(()=>[...l[0]||(l[0]=[t("h2",null,"Model token limits",-1),t("table",null,[t("thead",null,[t("tr",null,[t("th",null,"Model"),t("th",null,"Context Window"),t("th",null,"Use Case")])]),t("tbody",null,[t("tr",null,[t("td",null,"GPT-3.5-Turbo"),t("td",null,"4,096 tokens"),t("td",null,"Short conversations")]),t("tr",null,[t("td",null,"GPT-4"),t("td",null,"8,192 tokens"),t("td",null,"Standard applications")]),t("tr",null,[t("td",null,"GPT-4-32K"),t("td",null,"32,768 tokens"),t("td",null,"Long documents")]),t("tr",null,[t("td",null,"GPT-4-Turbo"),t("td",null,"128,000 tokens"),t("td",null,"Complex analysis")]),t("tr",null,[t("td",null,"Claude 3.5 Sonnet"),t("td",null,"200,000 tokens"),t("td",null,"Extended context")]),t("tr",null,[t("td",null,"Gemini 1.5 Pro"),t("td",null,"1,000,000 tokens"),t("td",null,"Massive documents")])])],-1)])]),_:1})]),_:1},16)}}};export{b as default};
