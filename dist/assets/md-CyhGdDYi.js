import{b as s,o as l,w as r,g as e,ad as i,v as u,x as c,S as o}from"./modules/vue-CJzrtd1M.js";import{I as d}from"./slidev/default-BA8wdPvG.js";import{u as m,f as p}from"./slidev/context-CYMzQU4q.js";import"./index-B5JIHzwN.js";import"./modules/shiki-BB3fXZko.js";const C={__name:"long_content_optimization.md__slidev_3",setup(f){const{$clicksContext:n,$frontmatter:a}=m();return n.setup(),(h,t)=>(l(),s(d,u(c(o(p)(o(a),2))),{default:r(()=>[...t[0]||(t[0]=[e("h1",null,"What Are Tokens?",-1),e("h2",null,"Definition",-1),e("p",null,'Tokens are the fundamental units that language models process. They represent the "vocabulary" that AI models understand.',-1),e("h2",null,"Token Characteristics:",-1),e("ul",null,[e("li",null,"Not equivalent to words or characters"),e("li",null,"Vary in length and composition"),e("li",null,"Model-specific (different models use different tokenization schemes)"),e("li",null,"The basis for all input/output limitations")],-1),e("p",null,[e("strong",null,"Critical Concept:"),i(" Language models donâ€™t read text as humans do. They process numerical token sequences.")],-1)])]),_:1},16))}};export{C as default};
